{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FromScratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our FromScratch CNN model to solve the checking_left_right_adjacency problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "#from collections import OrderedDict\n",
    "\n",
    "# generate random integer values\n",
    "from random import seed\n",
    "from random import randint\n",
    "import numpy as np\n",
    "#from pylab import array\n",
    "from random import sample\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "#from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "#import csv\n",
    "from time import time\n",
    "\n",
    "from Checking_adjacency_dataset import *\n",
    "from FromScratch_CNN import *\n",
    "from ResNetFT_Finetuning import *\n",
    "from Training_template import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset global variables\n",
    "my_root_dir = os.getenv(\"MY_ROOT_DIR\")\n",
    "my_val_dir = os.getenv(\"MY_VAL_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to False if you want to set the variables instead of using default\n",
    "default_setting_for_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = set_dataset_input(default_setting_for_dataset)\n",
    "my_sq_puzzle_piece_dim = data_inputs[0] \n",
    "my_size_of_buffer = data_inputs[1]\n",
    "my_model_dim = data_inputs[2]\n",
    "my_batch_size = data_inputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_sq_puzzle_piece_dim = 100\n",
      "my_size_of_buffer = 1000\n",
      "my_model_dim = 224\n",
      "my_batch_size = 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"my_sq_puzzle_piece_dim = {my_sq_puzzle_piece_dim}\")\n",
    "print(f\"my_size_of_buffer = {my_size_of_buffer}\")\n",
    "print(f\"my_model_dim = {my_model_dim}\")\n",
    "print(f\"my_batch_size = {my_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataloaders = create_dataloaders(my_root_dir,my_val_dir, my_sq_puzzle_piece_dim,\n",
    "                       my_size_of_buffer, my_model_dim,my_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing model type, hyperparameters and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 0 for FromScratch and 1 for ResNetFT 0\n",
      "************\n",
      "Using FromScratch\n",
      "feature_extracting : False\n"
     ]
    }
   ],
   "source": [
    "#Model details\n",
    "my_model_name, feature_extract = get_model_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FromScratch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "#Change this to False if you want to set the hyperparameters instead of using default\n",
    "default_setting_for_hyperparameters = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_learning_rate,my_momentum = get_hyperparameters(default_setting_for_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_learning_rate = 0.001\n",
      "my_momentum = 0.9\n"
     ]
    }
   ],
   "source": [
    "print(f\"my_learning_rate = {my_learning_rate}\")\n",
    "print(f\"my_momentum = {my_momentum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training epochs\n",
    "my_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating models, loss criterion and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FromScratch - Expect more number of parameters to learn!\n",
      "\t bigunit.0.conv1.weight\n",
      "\t bigunit.0.conv2.weight\n",
      "\t bigunit.0.unit.2.weight\n",
      "\t bigunit.0.unit.2.bias\n",
      "\t bigunit.0.unit.5.weight\n",
      "\t bigunit.0.unit.5.bias\n",
      "\t bigunit.1.conv1.weight\n",
      "\t bigunit.1.conv2.weight\n",
      "\t bigunit.1.unit.2.weight\n",
      "\t bigunit.1.unit.2.bias\n",
      "\t bigunit.1.unit.5.weight\n",
      "\t bigunit.1.unit.5.bias\n",
      "\t bigunit.2.conv1.weight\n",
      "\t bigunit.2.conv2.weight\n",
      "\t bigunit.2.unit.2.weight\n",
      "\t bigunit.2.unit.2.bias\n",
      "\t bigunit.2.unit.5.weight\n",
      "\t bigunit.2.unit.5.bias\n",
      "\t bigunit.3.conv1.weight\n",
      "\t bigunit.3.conv2.weight\n",
      "\t bigunit.3.unit.2.weight\n",
      "\t bigunit.3.unit.2.bias\n",
      "\t bigunit.3.unit.5.weight\n",
      "\t bigunit.3.unit.5.bias\n",
      "\t bigunit.4.conv1.weight\n",
      "\t bigunit.4.conv2.weight\n",
      "\t bigunit.4.unit.2.weight\n",
      "\t bigunit.4.unit.2.bias\n",
      "\t bigunit.4.unit.5.weight\n",
      "\t bigunit.4.unit.5.bias\n",
      "\t bigunit.5.conv1.weight\n",
      "\t bigunit.5.conv2.weight\n",
      "\t bigunit.5.unit.2.weight\n",
      "\t bigunit.5.unit.2.bias\n",
      "\t bigunit.5.unit.5.weight\n",
      "\t bigunit.5.unit.5.bias\n",
      "\t fc1.weight\n",
      "\t fc1.bias\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t fc2.weight\n",
      "\t fc2.bias\n",
      "\t bn2.weight\n",
      "\t bn2.bias\n",
      "No_of_parameters to learn : 44\n"
     ]
    }
   ],
   "source": [
    "my_model, my_loss_criterion, my_optimizer = make_model_lc_optimizer(my_model_name,\n",
    "                                                                    my_learning_rate, my_momentum,\n",
    "                                                                    feature_extract)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")   \n",
    "    print(\"Running on the GPU\")\n",
    "    #putting model on gpu\n",
    "    my_model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and displaying tensorboard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir=f\"Training_{my_model_name}\"\n",
    "my_board_writer = SummaryWriter(tensorboard_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-70416521db354d89\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-70416521db354d89\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"$tensorboard_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it(my_epochs, 0, \n",
    "        my_model_name, my_model, my_loss_criterion, my_optimizer,\n",
    "        my_batch_size, my_dataloaders,my_board_writer,device,batches_per_epoch=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We actually ran this notebook and trained the FromScratch model on Google Colabs so as to use GPU services._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a snapshot of the tensorboard after training the model for 25 epochs with 500 batches per epoch and batch-size 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training tensorboard](FromScratch_TrainingTensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Validation tensorboard](FromScratch_ValidationTensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing, we see that our model __FromScratch__ achieves around __95%__ validation accuracy after this preliminary training with validation negative log likelihood loss around __0.13__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
