{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the template for evaluating a model. We'll use this template to evaluate our checking_left_right_adjacency problem with _FromScratch_ and _ResNetFT_ models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "#from collections import OrderedDict\n",
    "\n",
    "# generate random integer values\n",
    "from random import seed\n",
    "from random import randint\n",
    "import numpy as np\n",
    "#from pylab import array\n",
    "from random import sample\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "#from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "#import csv\n",
    "from time import time\n",
    "\n",
    "from Checking_adjacency_dataset import *\n",
    "from FromScratch_CNN import *\n",
    "from ResNetFT_Finetuning import *\n",
    "from Training_template import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path where checkpoint was saved\n",
    "    model: model into which we want to load our checkpoint     \n",
    "    optimizer: optimizer into which we want to load our checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    min_validation_loss = checkpoint['min_validation_loss']\n",
    "    \n",
    "    return model, optimizer, checkpoint['epoch'], min_validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_it(no_of_epochs, starting_epoch, \n",
    "              model_name,model,loss_criterion, optimizer,\n",
    "              batch_size, dataloaders,board_writer,device,batches_per_epoch=100,\n",
    "              is_best=False,min_validation_loss=math.inf):\n",
    "\n",
    "    last_checkpoint_path = f\"./last_checkpoint_for_{model_name}.pt\"\n",
    "    best_model_path=f\"./best_model_for_{model_name}.pt\"\n",
    "    \n",
    "\n",
    "    for epoch in range(starting_epoch,starting_epoch+no_of_epochs):\n",
    "        print(f\"Epoch : {epoch}\")\n",
    "        start_time = time()\n",
    "\n",
    "        model.train()\n",
    "        print(\"Training\")\n",
    "        train_loss_in_this_epoch = 0\n",
    "        no_of_batches_in_this_epoch = 0\n",
    "        train_correct_in_this_epoch = 0\n",
    "        for train_batch_data, train_batch_labels in dataloaders[\"Training\"]:\n",
    "                train_batch_data, train_batch_labels = train_batch_data.to(device), train_batch_labels.to(device)\n",
    "                no_of_batches_in_this_epoch+= 1\n",
    "                optimizer.zero_grad()\n",
    "                train_batch_outputs = model(train_batch_data)\n",
    "                #Compute loss for this batch\n",
    "                train_batch_loss = loss_criterion(train_batch_outputs, train_batch_labels)\n",
    "                train_loss_in_this_batch = train_batch_loss.item()\n",
    "                train_loss_in_this_epoch += train_loss_in_this_batch \n",
    "                train_batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    train_score, train_predictions = torch.max(train_batch_outputs, axis = 1)   \n",
    "                    train_correct_in_this_batch = torch.sum(train_predictions == train_batch_labels.data).item()\n",
    "                    train_correct_in_this_epoch += train_correct_in_this_batch\n",
    "                if (no_of_batches_in_this_epoch % (batches_per_epoch//10)) == 0:\n",
    "                    print(f\"Training #{no_of_batches_in_this_epoch} Batch Acc : {train_correct_in_this_batch}/{batch_size}, Batch Loss: {train_loss_in_this_batch}\")\n",
    "                if no_of_batches_in_this_epoch == batches_per_epoch:\n",
    "                    print(f\"Epoch : {epoch}, Training Batch: {no_of_batches_in_this_epoch}\")\n",
    "                    break\n",
    "        board_writer.add_scalar(f'Training/Loss/Average', train_loss_in_this_epoch/no_of_batches_in_this_epoch, epoch)\n",
    "        board_writer.add_scalar(f'Training/Accuracy/Average', train_correct_in_this_epoch/(no_of_batches_in_this_epoch*batch_size), epoch)\n",
    "        board_writer.add_scalar(f'Training/TimeTakenInMinutes', (time()-start_time)/60, epoch)\n",
    "        board_writer.flush()\n",
    "        print(f\"Training average accuracy : {train_correct_in_this_epoch/(no_of_batches_in_this_epoch*batch_size)}\")\n",
    "        print(f\"Training average loss : {train_loss_in_this_epoch/no_of_batches_in_this_epoch}\")\n",
    "            \n",
    "\n",
    "        model.eval()\n",
    "        print(\"Validation\")\n",
    "        val_loss_in_this_epoch = 0\n",
    "        no_of_batches_in_this_epoch = 0\n",
    "        val_correct_in_this_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch_data, val_batch_labels in dataloaders[\"Validation\"]:\n",
    "                val_batch_data, val_batch_labels = val_batch_data.to(device), val_batch_labels.to(device)\n",
    "                no_of_batches_in_this_epoch+= 1\n",
    "                val_batch_outputs = model(val_batch_data)\n",
    "                #Compute loss for this batch\n",
    "                val_batch_loss = loss_criterion(val_batch_outputs, val_batch_labels)\n",
    "                val_loss_in_this_batch = val_batch_loss.item()\n",
    "                val_loss_in_this_epoch += val_loss_in_this_batch \n",
    "                val_score, val_predictions = torch.max(val_batch_outputs, axis = 1)   \n",
    "                val_correct_in_this_batch = torch.sum(val_predictions == val_batch_labels.data).item()\n",
    "                val_correct_in_this_epoch += val_correct_in_this_batch\n",
    "                if (no_of_batches_in_this_epoch % (batches_per_epoch//10)) == 0:\n",
    "                    print(f\"Validation #{no_of_batches_in_this_epoch} Batch Acc : {val_correct_in_this_batch}/{batch_size}, Batch Loss: {val_loss_in_this_batch}\")\n",
    "                if no_of_batches_in_this_epoch == batches_per_epoch:\n",
    "                    print(f\"Epoch : {epoch}, Validation Batch: {no_of_batches_in_this_epoch}\")\n",
    "                    break\n",
    "            board_writer.add_scalar(f'Validation/Loss/Average', val_loss_in_this_epoch/no_of_batches_in_this_epoch, epoch)\n",
    "            board_writer.add_scalar(f'Validation/Accuracy/Average', val_correct_in_this_epoch/(no_of_batches_in_this_epoch*batch_size), epoch)\n",
    "            board_writer.add_scalar(f'Validation/TimeTakenInMinutes', (time()-start_time)/60, epoch)\n",
    "            board_writer.flush()\n",
    "            print(f\"Validation average accuracy : {val_correct_in_this_epoch/(no_of_batches_in_this_epoch*batch_size)}\")\n",
    "            print(f\"Validation average loss : {val_loss_in_this_epoch/no_of_batches_in_this_epoch}\")\n",
    "            if  min_validation_loss >= val_loss_in_this_epoch:\n",
    "                    is_best = True\n",
    "                    min_validation_loss = min(min_validation_loss,val_loss_in_this_epoch)\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'min_validation_loss': min_validation_loss,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                    }\n",
    "                    save_checkpoint(checkpoint, is_best, last_checkpoint_path, best_model_path)\n",
    "                    print(f\"In epoch number {epoch}, average validation loss decreased to {val_loss_in_this_epoch/no_of_batches_in_this_epoch}\")\n",
    "            last_checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'min_validation_loss': min_validation_loss,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                     }\n",
    "            save_checkpoint(last_checkpoint, False, last_checkpoint_path, best_model_path)\n",
    "    board_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset global variables\n",
    "my_test_dir = os.getenv(\"MY_TEST_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to False if you want to set the variables instead of using default\n",
    "default_setting_for_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sq_puzzle_piece_dim,my_size_of_buffer,my_model_dim,my_batch_size = set_dataset_input(default_setting_for_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"my_sq_puzzle_piece_dim = {my_sq_puzzle_piece_dim}\")\n",
    "print(f\"my_size_of_buffer = {my_size_of_buffer}\")\n",
    "print(f\"my_model_dim = {my_model_dim}\")\n",
    "print(f\"my_batch_size = {my_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataloaders = create_dataloaders(my_root_dir,my_val_dir, my_sq_puzzle_piece_dim,\n",
    "                       my_size_of_buffer, my_model_dim,my_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing model type, hyperparameters and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model details\n",
    "my_model_names = ['','FromScratch', 'ResNetFT']\n",
    "feature_extract = get_model_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "#Change this to False if you want to set the hyperparameters instead of using default\n",
    "default_setting_for_hyperparameters = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_learning_rate,my_momentum = get_hyperparameters(default_setting_for_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"my_learning_rate = {my_learning_rate}\")\n",
    "print(f\"my_momentum = {my_momentum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training epochs\n",
    "my_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating models, loss criterion and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model, my_loss_criterion, my_optimizer = make_model_lc_optimizer(my_model_name,\n",
    "                                                                    my_learning_rate, my_momentum,\n",
    "                                                                    feature_extract)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")   \n",
    "    print(\"Running on the GPU\")\n",
    "    #putting model on gpu\n",
    "    my_model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and displaying tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir=f\"Training_{my_model_name}\"\n",
    "my_board_writer = SummaryWriter(tensorboard_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"$tensorboard_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it(my_epochs, 0, \n",
    "              my_model_name,my_model,my_loss_criterion, my_optimizer,\n",
    "              my_batch_size, my_dataloaders,my_board_writer,device,batches_per_epoch=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A template for the training pipeline was created. The working directory will contain two files called  __last_checkpoint_for_{my_model_name}.pt__ and __best_model_for_{my_model_name}.pt__. These are the saved checkpoint dictionaries of the last checkpoint and the best check point at the end of training. The best checkpoint will be used for prediction in the solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
