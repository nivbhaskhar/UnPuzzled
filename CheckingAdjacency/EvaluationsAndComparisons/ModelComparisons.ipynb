{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and comparisons of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three models to solve the __left-right-adjacency-problem__\n",
    "\n",
    "* AdjacencyClassifier_NoML \n",
    "* FromScratch\n",
    "* ResNetFT\n",
    "\n",
    "We evaulate these models on a test data set and compare their respective performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "#from collections import OrderedDict\n",
    "\n",
    "# generate random integer values\n",
    "from random import seed\n",
    "from random import randint\n",
    "import numpy as np\n",
    "#from pylab import array\n",
    "from random import sample\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "#from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "#import csv\n",
    "from time import time\n",
    "\n",
    "from Checking_adjacency_dataset import *\n",
    "from FromScratch_CNN import *\n",
    "from ResNetFT_Finetuning import *\n",
    "from Training_template import *\n",
    "from Adjacency_distance import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset global variables\n",
    "my_test_dir = os.getenv(\"MY_TEST_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to False if you want to set the variables instead of using default\n",
    "default_setting_for_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sq_puzzle_piece_dim 100\n",
      "Enter size_of_shuffle_buffer 1000\n",
      "Enter model_dim 224\n",
      "Enter batch_size 1000\n"
     ]
    }
   ],
   "source": [
    "data_inputs = set_dataset_input(default_setting_for_dataset)\n",
    "my_sq_puzzle_piece_dim = data_inputs[0] \n",
    "my_size_of_buffer = data_inputs[1]\n",
    "my_model_dim = data_inputs[2]\n",
    "my_batch_size = data_inputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_sq_puzzle_piece_dim = 100\n",
      "my_size_of_buffer = 1000\n",
      "my_model_dim = 224\n",
      "my_batch_size = 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"my_sq_puzzle_piece_dim = {my_sq_puzzle_piece_dim}\")\n",
    "print(f\"my_size_of_buffer = {my_size_of_buffer}\")\n",
    "print(f\"my_model_dim = {my_model_dim}\")\n",
    "print(f\"my_batch_size = {my_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adjacency_dataset = AdjacencyDataset(my_test_dir, my_sq_puzzle_piece_dim, \n",
    "                                        my_size_of_buffer, my_model_dim)\n",
    "test_adjacency_dataloader = DataLoader(test_adjacency_dataset, my_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "#Change this to False if you want to set the hyperparameters instead of using default\n",
    "default_setting_for_hyperparameters = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_learning_rate,my_momentum = get_hyperparameters(default_setting_for_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_threshold= 3.3036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_learning_rate = 0.001\n",
      "my_momentum = 0.9\n",
      "my_threshold = 0.9\n"
     ]
    }
   ],
   "source": [
    "print(f\"my_learning_rate = {my_learning_rate}\")\n",
    "print(f\"my_momentum = {my_momentum}\")\n",
    "print(f\"my_threshold = {my_momentum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GpuAvailable=True\n",
    "    my_device = torch.device(\"cuda:0\")   \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    GpuAvailable=False\n",
    "    my_device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and displaying tensorboard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir=\"Evaluation_50_1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-743b4e491b59ad08\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-743b4e491b59ad08\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6009;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"$tensorboard_dir\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "updated_model_names = [\"FromScratch\", \"ResNetFT\", \"AdjacencyClassifier_NoML\"]\n",
    "\n",
    "model_details = {}\n",
    "for i in range(2):\n",
    "    model_name=updated_model_names[i]\n",
    "    sub_dir = os.path.join(tensorboard_dir,model_name)\n",
    "    model_details[model_name]= {}\n",
    "    model_details[model_name]['writer']=SummaryWriter(sub_dir) \n",
    "    model_details[model_name]['no_ML']=False\n",
    "    model_details[model_name]['GpuAvailable']=GpuAvailable\n",
    "    model_details[model_name]['device']=my_device\n",
    "    model,loss_criterion,optimizer = make_model_lc_optimizer(model_name,\n",
    "                                                                my_learning_rate,\n",
    "                                                                my_momentum)\n",
    "    best_model_path=f\"./best_model_for_{model_name}.pt\"\n",
    "    model, optimizer, epochs_trained, min_val_loss = load_checkpoint_gpu(best_model_path,\n",
    "                                                                         model, \n",
    "                                                                         optimizer,\n",
    "                                                                         model_details[model_name]['GpuAvailable'])                                                           \n",
    "    model_details[model_name]['model']=model\n",
    "    model_details[model_name]['loss_criterion']=loss_criterion\n",
    "    model_details[model_name]['optimizer']=optimizer\n",
    "    if model_details[model_name]['GpuAvailable']:\n",
    "        model_details[model_name]['model'].to(my_device)\n",
    "    model_details[model_name]['test_batch_losses'] = [] \n",
    "    model_details[model_name]['test_batch_accuracies'] = [] \n",
    "    \n",
    "for i in range(2,3):\n",
    "    model_name=updated_model_names[i]\n",
    "    sub_dir = os.path.join(tensorboard_dir,model_name)\n",
    "    model_details[model_name]= {}\n",
    "    model_details[model_name]['writer']=SummaryWriter(sub_dir) \n",
    "    model_details[model_name]['no_ML']=True\n",
    "    model_details[model_name]['GpuAvailable']=False\n",
    "    model_details[model_name]['device']=torch.device(\"cpu\")\n",
    "    model_details[model_name]['model']=AdjacencyClassifier_NoML()\n",
    "    model_details[model_name]['loss_criterion']=None\n",
    "    model_details[model_name]['optimizer']=None\n",
    "    model_details[model_name]['test_batch_losses'] = [] \n",
    "    model_details[model_name]['test_batch_accuracies'] = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_step(model_name, test_batch_data, test_batch_labels):\n",
    "    my_board_writer = model_details[model_name]['writer']\n",
    "    model=model_details[model_name]['model']    \n",
    "    device=model_details[model_name]['device']\n",
    "    test_batch_data, test_batch_labels = test_batch_data.to(device), test_batch_labels.to(device)\n",
    "    if model_details[model_name]['no_ML']:\n",
    "        #Get predictions\n",
    "        test_predictions = model.predictions(test_batch_data, my_threshold)   \n",
    "    else:\n",
    "        test_batch_outputs = model(test_batch_data)\n",
    "        #Compute and plot loss for this batch\n",
    "        test_batch_loss = loss_criterion(test_batch_outputs, test_batch_labels)\n",
    "        test_loss_in_this_batch = test_batch_loss.item()\n",
    "        model_details[model_name]['test_batch_losses'].append(test_loss_in_this_batch)\n",
    "        my_board_writer.add_scalar(f'Test/BatchLoss/', test_loss_in_this_batch, no_of_batches)\n",
    "        #Get predictions\n",
    "        test_score, test_predictions = torch.max(test_batch_outputs, axis = 1) \n",
    "    #Compute and plot accuracy for this batch    \n",
    "    test_correct_in_this_batch = torch.sum(test_predictions == test_batch_labels.data).item()\n",
    "    test_avg_accuracy_in_this_batch = test_correct_in_this_batch/my_batch_size\n",
    "    model_details[model_name]['test_batch_accuracies'].append(test_avg_accuracy_in_this_batch)\n",
    "    my_board_writer.add_scalar(f'Test/AvgAccuracy/', test_avg_accuracy_in_this_batch, no_of_batches)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batches = 50\n",
    "no_of_batches = 0\n",
    "with torch.no_grad():\n",
    "        for test_batch_data, test_batch_labels in test_adjacency_dataloader:\n",
    "                no_of_batches+= 1\n",
    "                for model_name in updated_model_names:\n",
    "                    evaluate_step(model_name, test_batch_data, test_batch_labels)\n",
    "                \n",
    "                if no_of_batches == max_batches:\n",
    "                    break\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in updated_model_names:\n",
    "    model_details[model_name]['writer'].close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The evaluation was run on Google Colabs so as to use GPU services_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a snapshot of the evaluation tensorboard after testing each model on 50 batches with batch-size 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluation tensorboard](Comparison_of_evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ResNetFT's performance is very high with more than 99.8% accuracy consistently, while FromScratch performs with an accuracy varying between 94-99%. The AdjacencyClassifier_NoML underperforms in comparison varying between 84-95% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
