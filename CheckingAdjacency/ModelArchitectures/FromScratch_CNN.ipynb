{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Convolutional Neural Network from scratch\n",
    "\n",
    "\n",
    "Recall that we call a tuple of puzzle pieces (P, Q) (order matters!) to be __left-right adjacent__ if when P is placed to the left of Q, P's right edge is adjacent to Q's left edge.\n",
    "One idea is to use convolutional neural networks (CNNs). These try to retain the spatial structure of the inputs, which makes them work well on problems with images as inputs. \n",
    "\n",
    "We build a CNN network called _FromScratch_ to solve our __checking_left_right_adjacency_problem__.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# generate random integer values\n",
    "from random import seed\n",
    "from random import randint\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchvision import transforms, utils\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "#from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "#import csv\n",
    "from time import time\n",
    "\n",
    "\n",
    "import sys\n",
    "import Checking_adjacency_dataset as cad\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our earlier defined custom  _AdjacencyDataset_. Recall that we generate our data set from the CUB-200 dataset. \n",
    "\n",
    "__Input for AdjacencyDataset__\n",
    "<ul>\n",
    "    <li>root_dir : the root directory where the CUB-200 images are stored </li>\n",
    "<li> sq_puzzle_piece_dim : the dimension of the square puzzle piece (recall we cut the original image into uniform square puzzle pieces) </li>\n",
    "    <li> size_of_buffer : the buffer size for our shuffle_iterator</li>\n",
    "    <li> model_dim : input size for the model</li>\n",
    " </ul>\n",
    " \n",
    "__Output of AdjacencyDataset__\n",
    "<ul>\n",
    "    <li> juxtaposed_pieces_torchtensor : cropped (from the middle of the juxtaposed pieces) square rescaled piece with width, height = model_dim </li>\n",
    "    <li> label : 1 if left-right adjacent, 0 if not</li>\n",
    "</ul>\n",
    "\n",
    "Each data point therefore looks like (juxtaposed_pieces_torchtensor, label) the torchtensor has dimensions 3 x model_dim x model_dim (3 because RGB image, so 3 channels)\n",
    "The label is 1 if the pieces are left-right adjacent else 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_root_dir = os.getenv(\"MY_ROOT_DIR\")\n",
    "my_sq_puzzle_piece_dim = 100\n",
    "my_size_of_buffer = 1000\n",
    "my_model_dim = 224\n",
    "my_batch_size = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Mean computation\n",
    "\n",
    "To make the data have zero mean, we'd like to take the mean of all our input data-points and subtract it from the original data-points. The mean can be computed in various ways. Here, we'll take our full bird images and flatten those C x H x W tensors into long vectors, and just further, break them up into individual numbers and take that mean.\n",
    "\n",
    "i.e, each R or G or B pixel value is one number and we take the mean across all bird images of all these numbers. This is not really feasible, so we'll just lazily take as sample some bird images and find the mean of those bird images alone and assume they work well with the whole data-set.-->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch_adj_dataset = cad.AdjacencyDataset(my_root_dir, \n",
    "                                                      my_sq_puzzle_piece_dim, \n",
    "                                                      my_size_of_buffer, my_model_dim)\n",
    "\n",
    "\n",
    "\n",
    "train_from_scratch_adj_dataloader = DataLoader(train_from_scratch_adj_dataset, \n",
    "                                               my_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data point \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "juxtaposed_pieces_torchtensor, label = next(iter(train_from_scratch_adj_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 224, 224]), torch.Size([5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "juxtaposed_pieces_torchtensor.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(juxtaposed_pieces_torchtensor), torch.max(juxtaposed_pieces_torchtensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3728)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(juxtaposed_pieces_torchtensor.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture of \"FromScratch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the basic layers \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* __Convolutional filter (CF)__ : \n",
    "\n",
    "  A convolutional filter with kernel_size = (5,5) and stride = (1,1) and padding (2,2) and   no dilation. Such a filter turns input\n",
    "  __(in_channels x input_height x input_width)__ \n",
    "  into output __(1 x input_height x input_width)__. \n",
    "  That is, this convolutional filter doesn't change the height and width of input \n",
    "    \n",
    "    \n",
    "    \n",
    "* __Convolutional layer of shape C__ : \n",
    "\n",
    "    A convolutional layer with convolutional filters (CF) and further with no_of_filters  = in_channels. Thus after a convolutional layer of shape C , output dimension = input dimension.\n",
    "    \n",
    "\n",
    "* __Maxpool filter (MF)__ : \n",
    "\n",
    "  A maxpool filter with kernel_size = (2,2) and stride = (2,2) and padding 0 and no dilation. Such a filter turns input\n",
    "  __(in_channels x input_height x input_width)__ \n",
    "  into output __(in_channels x input_height//2 x input_width//2)__. \n",
    "    \n",
    "    \n",
    "* __Maxpool layer of shape M__ : \n",
    "\n",
    "    A maxpool layer with one maxpool filter MF. \n",
    "    \n",
    "    \n",
    "* __Batchnorm (B)__ :\n",
    "\n",
    "  _BatchNorm2d(in_channels)_ turns input tensor __(batchsize x in_channels x input_height x input_width)__  into output __(batchsize x in_channels x input_height x input_width)__. Batchnorms will be applied to the output of each layer so that the outputs are rescaled to fit into a nice (normal) distribution. \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "_A word about dilation_ : Dilation = n makes a pixel (1x1) of the kernel to be n x n, where the original kernel pixel is at the top left, and the rest pixels are empty (or filled with 0). Thus dilation=1 is equivalent to the standard convolution with no dilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A test Convolution layer of Shape C \n",
    "From PyTorch documentation, the following creates a convolution layer\n",
    "\n",
    "_torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=True, padding_mode='zeros')_\n",
    "\n",
    "where \n",
    "\n",
    "   * kernel and filter mean the same thing\n",
    "   * out_channels = number of filters, that is, depth of your output \n",
    "   * in_channels = depth of your input \n",
    "   \n",
    "     _Note that each filter has dimension in_channels x filter_height x filter_width. Thus when you run a filter over your input image, the filter's depth is the same as your image depth. So filter can move only across the width and height dimensions (2-D convolution)_\n",
    "     \n",
    "     \n",
    "  * The parameters kernel_size, stride, padding, dilation can either be a single int (in which case the same value is used for the height and width dimension) or a tuple of two ints (in which case, the first int is used for the height dimension, and the second int for the width dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_C_layer = nn.Conv2d(\n",
    "    in_channels = 3,\n",
    "    out_channels = 3,\n",
    "    kernel_size = (5,5),\n",
    "    stride = (1,1),\n",
    "    padding = (2,2),\n",
    "    dilation = 1,\n",
    "    groups = 1,\n",
    "    bias = False,\n",
    "    padding_mode = 'zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input to shape C layer__ : A (batchsize x in_channels x input_height x input_width) tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_to_shape_C_layer = torch.rand(my_batch_size,3,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_to_shape_C_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output of shape C layer__ : A (batchsize x out_channels x output_height x output_width) tensor where output_height=input_height and output_width=input_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_for_shape_C_layer = shape_C_layer(test_input_to_shape_C_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_for_shape_C_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A test Maxpool layer of Shape M\n",
    "\n",
    "\n",
    "From PyTorch documentation, the following creates a maxpool layer\n",
    "\n",
    "_torch.nn.MaxPool2d(kernel_size, stride, padding, dilation=1, return_indices=False, ceil_mode=False)_\n",
    "\n",
    "where \n",
    "\n",
    "   * kernel and filter mean the same thing\n",
    "   \n",
    "     _Note that each maxpool filter has dimension 1 x filter_height x filter_width. Thus when you run a filter over your input image, it will run across each input depth layer once_\n",
    "     \n",
    "     \n",
    "  * The parameters kernel_size, stride, padding, dilation can either be a single int (in which case the same value is used for the height and width dimension) or a tuple of two ints (in which case, the first int is used for the height dimension, and the second int for the width dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_M_layer = torch.nn.MaxPool2d(\n",
    "    kernel_size = (2,2),\n",
    "    stride=(2,2),\n",
    "    padding=0,\n",
    "    dilation=1, \n",
    "    return_indices=False,\n",
    "    ceil_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input to shape M layer__ : A (batchsize x in_channels x input_height x input_width) tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_to_shape_M_layer = torch.rand(my_batch_size,3,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_to_shape_M_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output of shape M layer__ : A (batchsize x out_channels x output_height x output_width) tensor where out_channels = in_channels, output_height=input_height//2 and output_width=input_width//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_for_shape_M_layer = shape_M_layer(test_input_to_shape_M_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 2, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_for_shape_M_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a BlockUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A BlockUnit U is built as follows:\n",
    "\n",
    "$$C_1\\to ReLU \\to B_1 \\to C_2 \\to ReLU \\to B_2 \\to M_1$$\n",
    "\n",
    "Here the $C_i$ are convolutional layers of shape C, the $B_i$ are BatchNorm layers and $M_1$ is a maxpool layer of shape M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockUnit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlockUnit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3, out_channels = 3, kernel_size = (5,5), stride = (1,1),\n",
    "            padding = (2,2), dilation = 1, groups = 1, bias = False, padding_mode = 'zeros')\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 3, out_channels = 3, kernel_size = (5,5), stride = (1,1),\n",
    "            padding = (2,2), dilation = 1, groups = 1, bias = False, padding_mode = 'zeros')\n",
    "        self.pool = torch.nn.MaxPool2d(\n",
    "            kernel_size = (2,2), stride=(2,2), padding=0, dilation=1,\n",
    "            return_indices=False, ceil_mode=False)\n",
    "        self.unit = nn.Sequential(\n",
    "            self.conv1, nn.ReLU(), nn.BatchNorm2d(3),\n",
    "            self.conv2,  nn.ReLU(), nn.BatchNorm2d(3),\n",
    "            self.pool)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batchsize x c x h x w > batchsize x c x (h//2) x (w//2)\n",
    "        return self.unit(x)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A test BlockUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input to BlockUnit__ : A (batchsize x in_channels x input_height x input_width) tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_to_blockunit = torch.rand(my_batch_size,3,5,10)\n",
    "test_input_to_blockunit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Output of BlockUnit__ : A (batchsize x out_channels x output_height x output_width) tensor where out_channels = in_channels, output_height=input_height//2 and output_width=input_width//2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 2, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_for_blockunit = BlockUnit()(test_input_to_blockunit)\n",
    "test_output_for_blockunit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FromScratch: The final model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model consists of some BlockUnits followed by two fully connected layers. More precisely, it is built as follows:\n",
    "\n",
    "\n",
    "$$I\\to U_1\\to U_2\\to U_3 \\to U_4 \\to U_5 \\to U_6 \\to FC_1 \\to ReLU \\to B_1 \\to FC_2 \\to B_2\\to ReLU\\to SoftMax$$\n",
    "\n",
    "\n",
    "Here the $U_i$ are the BlockUnits, the $B_i$ are BatchNorm layers and the $FC_i$ are the fully connected layers. \n",
    "\n",
    "\n",
    "_In the code, weâ€™ll actually apply and output the logsoftmax instead of the softmax as it is easier to handle sum of log of small numbers than products of small numbers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input-output dimensions__\n",
    "\n",
    "Let b =  batchsize, c = channels, h = height, w = width\n",
    "\n",
    "* Input from dataloader will be of dimension b,c,h,w = (b,3,224,224)\n",
    "* After passing through U_is, it will become (b, 3, 3, 3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FromScratch, self).__init__()\n",
    "        units = []\n",
    "        for i in [1,2,3,4,5,6]:\n",
    "            units.append(BlockUnit())\n",
    "        #   h > h/2 > h/4 > h/8 > h/16 > h/32 > h/64\n",
    "        #   w > w/2 > w/4 > w/8 > w/16 > w/32 > w/64\n",
    "        # input > 1  > 2  > 3  > 4 > 5 > 6\n",
    "        self.bigunit = nn.Sequential(*units)\n",
    "\n",
    "        \n",
    "        # 27 > 9\n",
    "        self.fc1 = nn.Linear(27,9)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        self.bn1 =  nn.BatchNorm1d(9)\n",
    "\n",
    "\n",
    "        # 9 > 2\n",
    "        self.fc2 = nn.Linear(9,2) \n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        self.bn2 =  nn.BatchNorm1d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x dim is 3 x 224 x 224\n",
    "        \n",
    "        \n",
    "        #Passing through 6 BlockUnits\n",
    "        #  224  > 112 > 56 > 28 > 14 > 7 > 3\n",
    "        # input > 1   > 2  > 3  > 4  > 5 > 6\n",
    "        x = self.bigunit(x)\n",
    "        # x dim now (batch_size, 3, 3, 3)\n",
    "        \n",
    "        \n",
    "        #Flattening x\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # x dim now (batch_size, 27)\n",
    "        \n",
    "        #Passing through FC1\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.bn1(x)\n",
    "        # x dim now (batch_size, 9)\n",
    "        \n",
    "        #Passing through FC1\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x= self.bn2(x)\n",
    "        # x dim (batch_size, 2)\n",
    "\n",
    "        x = nn.LogSoftmax(dim = 1)(x)\n",
    "        # x dim (batch_size, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "juxtaposed_pieces_torchtensor, label = next(iter(train_from_scratch_adj_dataloader))\n",
    "test_output_for_fromscratch = FromScratch()(juxtaposed_pieces_torchtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5749, -0.8273],\n",
       "        [-0.5749, -0.8273],\n",
       "        [-0.0619, -2.8138],\n",
       "        [-2.3423, -0.1010],\n",
       "        [-1.3246, -0.3091]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#log softmax outputs from the model\n",
    "test_output_for_fromscratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities=torch.exp(test_output_for_fromscratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5628, 0.4372],\n",
       "        [0.5628, 0.4372],\n",
       "        [0.9400, 0.0600],\n",
       "        [0.0961, 0.9039],\n",
       "        [0.2659, 0.7341]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities, predictions = torch.max(predicted_probabilities, axis = 1)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5628, 0.5628, 0.9400, 0.9039, 0.7341], grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built \"FromScratch\" our CNN model, whose outputs are vectors of the shape (a_0, a_1). We interpret $a_i = log p_i$, where $p_i$ is the model-predicted probability of the label being i (i=0 or 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FromScratch(\n",
      "  (bigunit): Sequential(\n",
      "    (0): BlockUnit(\n",
      "      (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (unit): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (4): ReLU()\n",
      "        (5): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BlockUnit(\n",
      "      (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (unit): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (4): ReLU()\n",
      "        (5): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (2): BlockUnit(\n",
      "      (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (unit): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (4): ReLU()\n",
      "        (5): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (3): BlockUnit(\n",
      "      (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (unit): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (4): ReLU()\n",
      "        (5): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (4): BlockUnit(\n",
      "      (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (unit): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (4): ReLU()\n",
      "        (5): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (5): BlockUnit(\n",
      "      (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (unit): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (4): ReLU()\n",
      "        (5): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=27, out_features=9, bias=True)\n",
      "  (bn1): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=9, out_features=2, bias=True)\n",
      "  (bn2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(FromScratch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
